
BigCache after 4 years
every milisecond counts
18:03 2 Feb 2019
Tags: go, slices, bug

Tomasz Janiszewski
Software Engineer, D2iQ
janiszt@gmail.com
@janiszt

: Hello, I'm Tomasz and today I'll tell you about a project that I'm involved for last 4 years.
: Nevertheless, I'm not working in the company that started this project.
: Everything is possible due to open source.

* What is BigCache?

- fast concurent hashmap
- minimal GC footprint
- super simple – not LRU

  func (c *BigCache) Get(key string) ([]byte, error)

  func (c *BigCache) Set(key string, entry []byte) error

* Why?

- Initially: to store request metadata for short period
- Now: for fun :)

* Why?

.image why.png


* When?

  $ git show `git rev-list --max-parents=0 HEAD`
  commit 7a649d1ced34a6fec7904cba51f7db300f14e498
  Author: Adam Dubiel <adamdubiel@users.noreply.github.com>
  Date:   Wed Mar 23 08:18:52 2016 +0100
      Initial commit


* Does it perform well?

.image cache_write_workload.svg
.caption https://blog.dgraph.io/post/caching-in-go/

* Does it perform well?

.image cache_read_workload.svg
.caption https://blog.dgraph.io/post/caching-in-go/

* Does it perform well?

.image cache_mixed_workload.svg
.caption https://blog.dgraph.io/post/caching-in-go/


* Does it perform well?

YES*

*** Do your own benchmarks!

* How?

- RWMutex
- Shards
- Bytes queue
- No `defer`
- Bitwise modulo
- Buffer
- no alloc
- A little copying
- Zero dependency

* Start with single map with Mutex

* RWMutex

  type RWMutex struct {
  	w           Mutex  // held if there are pending writers
  	writerSem   uint32 // semaphore for writers to wait for completing readers
  	readerSem   uint32 // semaphore for readers to wait for completing writers
  	readerCount int32  // number of pending readers
  	readerWait  int32  // number of departing readers
  }

- concurrent reads are nearly blocking free
- writes wait for all reads and other writes

  // Wait for active readers.
  if r != 0 && atomic.AddInt32(&rw.readerWait, r) != 0 {
    runtime_SemacquireMutex(&rw.writerSem, false, 0)
  }

* Shards – improve concurrency

- aka map of maps (implemented with array)
- split single map into smaller maps
- hash(key)%len(shards)

* Bitwise operation instead of modulo

  // x % (2 << n) == x & (2 << n - 1)
  func (c *BigCache) getShard(hashedKey uint64) (shard *cacheShard) {
  	return c.shards[hashedKey&c.shardMask]
  }

.link https://github.com/allegro/bigcache/pull/5

* Bitwise operation instead of modulo

.code x_test.go

* .
.background modulo.png


* Bytes queue – reduce GC

.image large_map.png

.link https://github.com/golang/go/issues/9477

.html gc_map.html

* .
.background maps.png

* Bytes queue – reduce GC

- shard is map[int]int (map[hash(key)]index)
- array of maps ([]map[int]int) instead of map of maps (map[int]map[int]int)
- values are stored in huge array (queue)

* Bytes queue – reduce GC


* Never defer...


* ..but

.image defer.png

* No alloc...

  func bytesToString(b []byte) string {
  	bytesHeader := (*reflect.SliceHeader)(unsafe.Pointer(&b))
  	strHeader := reflect.StringHeader{Data: bytesHeader.Data, Len: bytesHeader.Len}
  	return *(*string)(unsafe.Pointer(&strHeader))
  }

.link https://github.com/allegro/bigcache/pull/24

* ... but

  go-app-builder: Failed parsing input: parser: bad import "unsafe"
                in github.com/allegro/bigcache/bigcache.go from GOPATH

-  Clear is better than clever.
-  Reflection is never clear.

.link https://go-proverbs.github.io/

* A little copying

Copy FNV hash code to reduce allocation

.link https://github.com/allegro/bigcache/pull/19

* Zero dependency

.html pike.html
